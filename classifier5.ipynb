{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "756d12f8-9ad5-4e40-9187-135e4946287e",
   "metadata": {},
   "source": [
    "# Polar Ring Galaxy Example: NGC 660\n",
    "\n",
    "Here is an example of a polar ring galaxy, NGC 660, the type this project focuses on identifying.\n",
    "\n",
    "![NGC 660 Polar Ring Galaxy](https://upload.wikimedia.org/wikipedia/commons/thumb/9/9d/NGC_660_Polar_Galaxy_Gemini_Observatory.jpg/800px-NGC_660_Polar_Galaxy_Gemini_Observatory.jpg)\n",
    "\n",
    "*Image Credit: Gemini Observatory/AURA*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77eeb7ff-5e8c-4906-8c52-f4dfd151cbc8",
   "metadata": {},
   "source": [
    "# Galaxy Classification\n",
    "This notebook uses the fastai package (built on PyTorch) as well as the sklearn package to create a supervised machine learning model to classify images of galaxies. The data for this project came from the Hyper-Suprime Cam Subary Sky Survey (https://hsc.mtk.nao.ac.jp/ssp/). Images were manually classified into categories.\n",
    "\n",
    "This notebook also makes sue of Nvidia CUDA, a program which, in this case, allows the model to directly utilize the CUDA cores of a PC's Graphics Processing Unit (GPU). This increases the speed of the model as the GPU can more effectively handle batches of galaxy images than the CPU can.\n",
    "\n",
    "If importing an already created model, skip down to the \"Import Model\" section. Make sure you run the imports below though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58605a1e-0172-4bdd-b036-2e4f9b6319a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from shutil import copy\n",
    "from fastai.vision.all import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "# ONLY USE BELOW CODE IF USING GPU FOR ADDITONAL HORSEPOWER\n",
    "print(torch.cuda.is_available())  # Should print True if GPU is enabled\n",
    "print(torch.cuda.get_device_name(0))  # Shows the name of your GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accf8dac-c802-485a-8e81-62c1ff9374c6",
   "metadata": {},
   "source": [
    "## Quick pre-processing\n",
    "\n",
    "Not a lot of pre-processing is necessary. The only aspect that is important is to ensure that no truncated images (images that won't open properly) make it into either the training session or the final data classification, or else the model will crash and waste a lot of time. Alternatively, a try() catch() could be used in the final model, but I prefer to just remove the trouble images outright to prevent any trouble. The following code will identify and delete images that do not open.\n",
    "\n",
    "This code is not necessary if you know your images are correct, but is probably acceptable to use if you are uncertain or if it is your first time with those images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8162512a-1616-423d-9416-c1e49c74ec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Path to your dataset\n",
    "dataset_path = Path('ims')\n",
    "\n",
    "# List to store corrupted file paths\n",
    "corrupted_files = []\n",
    "\n",
    "# Check all image files in the folder\n",
    "for img_path in dataset_path.iterdir():\n",
    "    try:\n",
    "        # Try to open the image\n",
    "        img = Image.open(img_path)\n",
    "        img.verify()  # Verify integrity\n",
    "    except Exception as e:\n",
    "        print(f\"Corrupted file: {img_path}, Error: {e}\")\n",
    "        corrupted_files.append(img_path)  # Save the corrupted file path to the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9d42eb-ecc8-4eac-9370-4a1814ffdcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete all corrupted files\n",
    "for file_path in corrupted_files:\n",
    "    try:\n",
    "        file_path.unlink()  # Deletes the file\n",
    "        print(f\"Deleted: {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to delete {file_path}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90df7d4-5b8a-4205-b7a2-50efe7d92a38",
   "metadata": {},
   "source": [
    "## Create training sets\n",
    "\n",
    "This section splits the data in the training folder (variable source_folder) using train_test_split from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f4219-9456-4a05-b1b7-c7c81c49a986",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split test and train data randomly\n",
    "source_folder = Path('training data')\n",
    "classes = [folder.name for folder in source_folder.iterdir() if folder.is_dir() and folder.name not in ['train', 'valid', 'models']]\n",
    "\n",
    "\n",
    "for cls in classes:\n",
    "    img_path = source_folder / cls\n",
    "    images = list(img_path.iterdir())\n",
    "    \n",
    "    # Split into train and valid\n",
    "    train_imgs, valid_imgs = train_test_split(images, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Create target folders\n",
    "    (source_folder / 'train' / cls).mkdir(parents=True, exist_ok=True)\n",
    "    (source_folder / 'valid' / cls).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Move images\n",
    "    for img in train_imgs:\n",
    "        shutil.copy(str(img), str(source_folder / 'train' / cls / img.name))\n",
    "    for img in valid_imgs:\n",
    "        shutil.copy(str(img), str(source_folder / 'valid' / cls / img.name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8c7939-f793-4c4d-b4eb-eb17ba2839ca",
   "metadata": {},
   "source": [
    "## Create learning model\n",
    "\n",
    "The following cell defines our dls and directs it to the training data folder. This is where different parameters can be set up. I have found that a bs of 64 works well on an RTX 5070 Ti but will likely need to be toned down if using a GPU with lower VRAM. VRAM is the largest limitation in training times. The architecture can be changed in the vision_learner function at the bottom. ConvNeXt_tiny has shown an improvement in accuracy over ResNet. It seems to be on-par with the larger ConvNeXt models, and is significantly faster as well.\n",
    "\n",
    "The cell after this one finds the optimal learning rate. Probably not necessary, but I believe that adaptability is always good, so I like to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcbfac2-4fe2-4a16-b22f-60713909b212",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load train data\n",
    "source_folder = Path('training data')\n",
    "dls = ImageDataLoaders.from_folder(\n",
    "    source_folder, # Trains on pre-defined source folder above (training data)\n",
    "    train='train', \n",
    "    valid='valid',\n",
    "    num_workers=12, #Sets CPU to use specified workers (I am using a Ryzen 9 5900X with 12 cores, trying to maximize power) \n",
    "    bs=64, #Sets batch size to bs to leverage GPU in processing by increasing batch size, or decreasing if VRAM runs out\n",
    "    pin_memory=True #Improves data transfer between CPU and GPU\n",
    ") \n",
    "#, item_tfms=Resize(224)) #Resize images (speeds up model but decreases accuracy (probably) because image looks worse)\n",
    "\n",
    "#Create model\n",
    "learn = vision_learner(dls, convnext_tiny, metrics=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e769f1-d726-446b-a557-efc0f47363e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find optimal learning value instead of guessing\n",
    "optimal_model = learn.lr_find()\n",
    "print(optimal_model)\n",
    "optimal_rate = optimal_model.valley"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ee727-eb9b-42da-8370-7412a68d979b",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "Simple training using fine_tune. Has an early stop if valid_loss either drops or stops improving 3 epochs in a row. If using a powerful GPU, no reason to not just let the model go for as long as it wants, as it can catch itself.\n",
    "\n",
    "Cell afterwards creates a confusion matrix to show how training went. Optional use.\n",
    "\n",
    "Finally, if the model was good and you want to save time on re-training, there's a line to export the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673fdcee-b6ad-4685-a154-e2cc8c052200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fine tune\n",
    "learn.fine_tune(\n",
    "    5, #epochs (# of trials)\n",
    "    cbs=[\n",
    "        EarlyStoppingCallback(monitor='valid_loss', patience=3), #Stops training epochs if valid_loss begins dropping\n",
    "        SaveModelCallback(monitor='valid_loss', fname='best_model') #Saves the best model if training stops\n",
    "    ],\n",
    "    base_lr = optimal_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8173293-a3e6-44b2-9e0e-b8240d88f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate model with confusion matrix (not necessary but useful visual)\n",
    "interp = ClassificationInterpretation.from_learner(learn)\n",
    "interp.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd48560-d3a6-470f-904c-8b4877f2b299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save classifier\n",
    "learn.export('full_model2.8.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e48d40d-0116-4df9-ac42-60ce2394dd05",
   "metadata": {},
   "source": [
    "# Import Model and Full Dataset Test\n",
    "\n",
    "If model is already created, skip to this point to import and run on dataset.\n",
    "\n",
    "Some trial and error was necessary to get the model to utilize the GPU properly here, hence the cpu=False line that is commented out. I left it in just in case of emergency, but the next two lines to move the model to 'cuda' should work. If the following two print lines do not print True and cuda, then try cpu=False in the load_learner line after model name. This problem only seems to pop up if importing a model. and works perfectly fine if going straight through with creation of a model above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8b49c-6ffd-40f1-ae75-bdae857575b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in learner\n",
    "learn = load_learner('full_model2.8.pkl') #cpu=False\n",
    "dls = learn.dls\n",
    "print(\"Model loaded.\")\n",
    "learn.model.to('cuda')\n",
    "learn.dls.device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252d098-e376-4cd0-a6f6-c6cd65643fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(learn.model.parameters()).is_cuda)\n",
    "print(learn.dls.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d092f0-9f51-4933-bcb3-262f7054418d",
   "metadata": {},
   "source": [
    "### Run Model on Full Dataset\n",
    "\n",
    "Create path to dataset we want to classify, then define dls again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0443fd8-3cba-4be2-94e0-5eff6405d2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Path to large dataset\n",
    "# full_dataset = Path('ims')\n",
    "full_dataset = Path('small_test_ims')\n",
    "image_files = list(full_dataset.iterdir())\n",
    "\n",
    "test_dls = learn.dls.test_dl(image_files, pin_memory=True) #This is a good point to edit the aspects of the model before testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d65ece9-0515-4587-9553-e79898334cb1",
   "metadata": {},
   "source": [
    "#### Creating Probabilities and Predicting Classes\n",
    "\n",
    "At least for the time being, the model seems to like to create a cap on its predictive probability (around 0.23). I suspect this is due to suspect training data and the model lacks confidence in its answers. When examining the logits, they are found to be approximately the same number except for the predicted answer which is a bit higher. I believe this to be fixable with fixing up the training sets, but until then, temperature scaling is used in the temperature_softmax function to simply scale the probabilites in the creation of the catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c18ad9a-fb1c-4165-baca-f7f7df1bec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def temperature_softmax(logits, T=0.5):\n",
    "    return F.softmax(logits / T, dim=1)\n",
    "\n",
    "# Get raw logits from your model\n",
    "preds, _ = learn.get_preds(dl=test_dls)\n",
    "\n",
    "# Try with a temperature\n",
    "probs_temp = temperature_softmax(preds, T=0.28)\n",
    "\n",
    "# Save probabilities along with class predictions\n",
    "predicted_classes = [learn.dls.vocab[i] for i in probs_temp.argmax(dim=1)]\n",
    "probs_list = probs_temp.tolist()  # Convert tensor to a Python list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd869aba-0846-419c-9780-b225343c6968",
   "metadata": {},
   "source": [
    "### Catalog Creation\n",
    "\n",
    "Saves a catalog of all predicted images with their file name, predicted class, and probability of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92c070d-49e7-4709-8105-2bdb74da5af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame of probabilities with each column named after the corresponding class\n",
    "prob_df = pd.DataFrame(probs_temp.numpy(), columns=learn.dls.vocab)\n",
    "\n",
    "# Retrieve filenames directly from the test DataLoader's dataset (assuming items are file paths)\n",
    "filenames = [Path(item).name for item in test_dls.dataset.items]\n",
    "\n",
    "# Create the catalog DataFrame\n",
    "catalog = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'predicted_class': predicted_classes\n",
    "})\n",
    "\n",
    "# Concatenate the probabilities: one column per galaxy type\n",
    "catalog = pd.concat([catalog, prob_df], axis=1)\n",
    "\n",
    "# Round probabilities\n",
    "catalog.update(catalog.select_dtypes(float).round(8))\n",
    "\n",
    "# Save the catalog to a CSV file\n",
    "catalog.to_csv('catalog_small.csv', index=False)\n",
    "print(\"Catalog saved as catalog_small.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
